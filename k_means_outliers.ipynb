{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up analysis and load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "GLOVE_FILE = \"glove-100-angular.hdf5\"\n",
    "STACKOVERFLOW_FILE = \"stackoverflow-512-angular.hdf5\"\n",
    "IMAGENET_FILE = \"imagenet-96-angular.hdf5\"\n",
    "\n",
    "num_queries = 100\n",
    "k = 10\n",
    "\n",
    "quantiles = (0.0, 0.1, 0.5, 0.9, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape: (1000000, 512)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def load_data(filename):\n",
    "    dataset = h5py.File(filename, 'r')\n",
    "\n",
    "    train_data = np.array(dataset['train'])\n",
    "    test_data = np.array(dataset['test'])[:num_queries]\n",
    "\n",
    "    train_data = preprocessing.normalize(train_data, axis=1, norm='l2')\n",
    "    test_data = preprocessing.normalize(test_data, axis=1, norm='l2')\n",
    "    return (train_data, test_data)\n",
    "\n",
    "(train_data, test_data) = load_data(STACKOVERFLOW_FILE)\n",
    "print('train_data.shape:', train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def run_kmeans(data, n_centroids):\n",
    "    sample_size = int(len(data) / 10)\n",
    "\n",
    "    sample = np.random.permutation(data)[:sample_size]\n",
    "    sample = sample.astype(np.float32)\n",
    "\n",
    "    kmeans = faiss.Kmeans(data.shape[1], n_centroids, niter=20, verbose=True)\n",
    "    kmeans.train(data)\n",
    "    return kmeans\n",
    "\n",
    "def compute_cluster_stats(model, data):\n",
    "    sample_size = int(len(data) / 10)\n",
    "    sample = np.random.permutation(data)[:sample_size]\n",
    "    \n",
    "    dists = []\n",
    "    other_dists = []\n",
    "    centroid_dists = []\n",
    "    \n",
    "    D, I = model.index.search(sample, len(model.centroids))\n",
    "    for i, v in enumerate(sample):\n",
    "        dists.append(np.sqrt(D[i][0]))\n",
    "        other_dists.extend(np.sqrt(D[i][1:]))\n",
    "        \n",
    "    D, I = model.index.search(model.centroids, len(model.centroids))\n",
    "    for i, v in enumerate(model.centroids):\n",
    "        centroid_dists.extend(np.sqrt(D[i][1:]))\n",
    "        \n",
    "    return (dists, other_dists, centroid_dists)\n",
    "\n",
    "def compute_cluster_counts(model, data):\n",
    "    counts = [0] * len(model.centroids)\n",
    "    \n",
    "    D, I = model.index.search(data, 1)\n",
    "    for i, v in enumerate(data):\n",
    "        centroid = I[i][0]\n",
    "        counts[centroid] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How robust is k-means to outliers (from another distribution)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import neighbors\n",
    "\n",
    "PCA_DIMS = 100\n",
    "np.random.seed(0)\n",
    "\n",
    "pca = decomposition.PCA(n_components=PCA_DIMS)\n",
    "train_data = pca.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(other_train_data, other_test_data) = load_data(GLOVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_outliers(data, other_data, percentage):\n",
    "    sample_size = int(data.shape[0] * percentage)\n",
    "    sample = np.random.permutation(other_data)[:sample_size]\n",
    "    \n",
    "    indices = tuple([np.random.randint(data.shape[0], size=sample_size)])\n",
    "    new_data = np.copy(data)\n",
    "    new_data[indices] = sample\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0.0      0.1      0.5       0.9       1.0\n",
      "dist to nearest centroid  0.175    0.343    0.449     0.601     1.011\n",
      "dist to other centroids   0.223    0.665    0.824     0.983     1.614\n",
      "dist between centroids    0.178    0.526    0.686     0.827     1.500\n",
      "cluster sizes             1.000  472.700  950.500  1588.500  3070.000\n"
     ]
    }
   ],
   "source": [
    "n_centroids = 1000\n",
    "index = ['dist to nearest centroid', 'dist to other centroids', 'dist between centroids', 'cluster sizes']\n",
    "\n",
    "percentage = 0.01\n",
    "new_data = add_outliers(train_data, other_train_data, percentage)\n",
    "\n",
    "kmeans = run_kmeans(new_data, n_centroids)\n",
    "(dists, other_dists, centroid_dists) = compute_cluster_stats(kmeans, new_data)\n",
    "counts = compute_cluster_counts(kmeans, new_data)\n",
    "\n",
    "output = [np.quantile(dists, quantiles),\n",
    "          np.quantile(other_dists, quantiles),\n",
    "          np.quantile(centroid_dists, quantiles),\n",
    "          np.quantile(counts, quantiles)]\n",
    "df = pd.DataFrame(output, index=index, columns=quantiles)\n",
    "print(df.round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1 for c in counts if c < 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How robust is k-means to outliers (random vectors)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = train_data.shape[0]\n",
    "p = train_data.shape[1]\n",
    "\n",
    "random_data = np.random.normal(0, 1, n * p).reshape(n, p)\n",
    "random_data = preprocessing.normalize(random_data, axis=1, norm='l2')\n",
    "random_data = random_data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_centroids = 1000\n",
    "index = ['dist to nearest centroid', 'dist to other centroids', 'dist between centroids', 'cluster sizes']\n",
    "\n",
    "percentage = 0.01\n",
    "new_data = add_outliers(train_data, random_data, percentage)\n",
    "\n",
    "kmeans = run_kmeans(new_data[0:1000000,:], n_centroids)\n",
    "(dists, other_dists, centroid_dists) = compute_cluster_stats(kmeans, new_data[0:1000000,:])\n",
    "counts = compute_cluster_counts(kmeans, new_data)\n",
    "\n",
    "output = [np.quantile(dists, quantiles),\n",
    "          np.quantile(other_dists, quantiles),\n",
    "          np.quantile(centroid_dists, quantiles),\n",
    "          np.quantile(counts, quantiles)]\n",
    "df = pd.DataFrame(output, index=index, columns=quantiles)\n",
    "print(df.round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1 for c in counts if c < 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
